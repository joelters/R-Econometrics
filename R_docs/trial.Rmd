---
title: "aaa"
author: "bbb"
output:
  html_document:
    theme: yeti
    toc: true
    toc_float: true
bibliography: references.bib
---

# Average Treatment Effects

## Random Treatment

Suppose that we have some treatment $D_i \in \{0,1\}$ and an observed outcome $Y_i = D_i Y(1)_i + (1 - D_i)*Y(0)_i$ where $Y(D_i)_i$ is a potential outcome: the outcome individual $i$ gets under $D_i$. Since in our data individuals are either treated or not, there is always an element of the pair $(Y(1)_i,Y(0)_i)$ which we do not observe. If the potential outcomes are independent from treatment $A1: (Y(1)_i,Y(0)_i) \perp D_i$ (e.g. if the treatment is randomized) then we can identify what we call the Average treatment effect (ATE):

$$
\tau_0 = \mathbb{E}[Y(1)_i - Y(0)_i] = \mathbb{E}[Y_i | D_i = 1] - \mathbb{E}[Y_i | D_i = 0] ,
$$
where the second equality follows from A1. Let us load data used in [@lalonde1986evaluating] and [@dehejia1999causal] of a random experiment (National Supported Work (NSW)) where job training is assigned at random among a group of disadvantaged individuals. We also clean the workspace and load some packages we are going to need in this section (we download the package containing the data from github using devtools, check the link for more info).


```{r, warning=FALSE, message=FALSE}
rm(list = ls())
library(devtools)
library(dplyr)
library(lmtest)
library(matlib)
library(ranger)
#download package from github see https://github.com/jjchern/lalonde
devtools::install_github("jjchern/lalonde")

exp_df <- as_tibble(lalonde::nsw)
print(exp_df)
```

We are interested in real earnings in 1978 (re78) since in 1978 the program had already finished. Since this data is experimental we can directly estimate the ATE by looking at the difference in sample means

$$
\hat{\tau} = \frac{\sum_{i = 1}^n D_i Y_i}{\sum_{i = 1}^n D_i} - \frac{\sum_{i = 1}^n (1-D_i) Y_i}{\sum_{i = 1}^n (1-D_i)} = \frac{1}{n_1}\sum_{i: D_i = 1} Y_i - \frac{1}{n_0}\sum_{i: D_i = 0} Y_i, 
$$
where $n_j = \sum_{i=1}^n 1(D_i = j)$ for $j = 0,1$. We can also estimate the standard error as

$$
\hat{se}(\hat{\tau}) = \sqrt{\frac{\hat{\sigma}_1^2}{n_1} + \frac{\hat{\sigma}_0^2}{n_0}},
$$
where $\hat{\sigma}_j = n_j^{-1} \sum_{i: D_i = j} (Y_i - \bar{Y}_j)^2$ where $\bar{Y}_j = n_j^{-1} \sum_{i: D_i = j} Y_i$. In R

```{r}
tauhat <- mean(exp_df$re78[exp_df$treat == 1]) - mean(exp_df$re78[exp_df$treat == 0])
sehat <- sqrt(var(exp_df$re78[exp_df$treat == 1])/sum(exp_df$treat == 1) + 
              var(exp_df$re78[exp_df$treat == 0])/sum(exp_df$treat == 0))
c("tauhat" = tauhat, "se" = sehat)
```

Alternatively, we can estimate the ATE by a regression, letting $\alpha = \mathbb{E}[Y_i | D_i = 0]$ and $\varepsilon_i = Y_i - \mathbb{E}[Y_i|D_i]$ we have that

$$
Y_i = \mathbb{E}[Y_i |D_i] + Y_i - \mathbb{E}[Y_i |D_i] =  \alpha + \tau D_i + \varepsilon_i.
$$
where by construction $\mathbb{E}[\varepsilon_i |D_i] = 0$. Hence we can estimate $\hat{\tau}$ by OLS and the standard errors by the common robust OLS standard errors and we get the same

```{r}
m1exp <- lm(re78 ~ treat, exp_df)
lmtest::coeftest(m1exp, vcov=sandwich::vcovHC(m1exp, type='HC2'))[2,]
```

The regression approach is useful since it allows to adjust for covariates. This means adding other covariates $X_i$ as regressors. Since $D_i$ is randomly assigned, $X_i$ will be independent of $D_i$ and hence will not affect the parameter associated to $D_i$, also, if $X_i$ has explanatory power over $Y_i$ it will increase the precision of our ATE estimator by reducing the variance of the residuals. For instance, following [@lalonde1986evaluating] and [@dehejia1999causal] we can adjust for age, age squared, education, race and high school degree. 


```{r}
m2exp <- lm(re78 ~ treat + poly(age,2) + education + black + hispanic + nodegree, exp_df)
lmtest::coeftest(m2exp, vcov=sandwich::vcovHC(m2exp, type='HC2'))[2,]
``` 

The unadjusted ATE was 886 dollars, meaning that real earnings of treated individuals in 1978 were 886 dollars higher than the real earnings of the control. When adjusting for covariates the ATE decreases slightly to almost 800 dollars. We see that the adjusted estimate is more precise although less significant since it is closer to zero.

## Selection on observables

The main idea in [@lalonde1986evaluating] and [@dehejia1999causal] is to replace the experimental control group with a control group which is taken from a survey. This means that we go from an experimental setting to an observational one (one where the potential outcomes cannot be expected to be independent from the treatment). The goal is to see whether observational methods can get near to the experimental estimates. To construct this database we keep only the treated from the experimental data and take as control the Current Population Survey (CPS).

```{r}
cps_lalonde <- lalonde::cps_controls2
obs_df <- exp_df %>%
  filter(treat == 1) %>% 
  bind_rows(cps_lalonde) %>% 
  dplyr::select(-data_id)
```

To be able to identify treatment effects we still need to assume selection on observables: $A1': (Y(1)_i, Y(0)_i) \perp D_i | X_i$. That is, the distribution of the potential outcomes now can vary for individuals with different treatments as long as they have different covariates, however, for those with the same covariates, the potential outcomes still have to be independent from treatment. Let us define an ATE which might depend on $X_i$ and denote it as $ATE(X_i)$ (also called CATE)

$$
ATE(X_i) = \mathbb{E}[Y(1)_i| X_i] - \mathbb{E}[Y(0)_i |X_i].
$$
To retrieve the ATE we can just take $\mathbb{E}[ATE(X_i)]$. By A1' we have that

$$
ATE(X_i) = \mathbb{E}[Y_i | D_i = 1, X_i] - \mathbb{E}[Y_i | D_i = 0, X_i].  
$$

By specifying some regression $m_1(X_i) = \mathbb{E}[Y_i | D_i = 1, X_i]$ and $m_0(X_i) = \mathbb{E}[Y_i | D_i = 0, X_i]$ and estimating the regressions we get what is sometimes called a **direct estimator**. If the ATE is constant across the support of $X_i$, then $ATE = ATE(X_i)$. One case in which this is true is under the assumption that $\mathbb{E}[Y_i | D_i, X_i] = \delta_0 + \tau D_i + \delta_1' X_i$. Under this linearity assumption, the ATE can be estimated by OLS

```{r}
m1obs <- lm(re78 ~ treat + poly(age,2) + education + black + hispanic + nodegree, obs_df)
lmtest::coeftest(m1obs, vcov=sandwich::vcovHC(m1obs, type='HC2'))[2,]
```
we see that the estimated ATE is very negative. This is because the treatment and the control are hardly comparable. The treatment is formed by disadvantaged individuals and the control by a representative sample of the US population. In the absence of treatment disadvantaged individuals would already have much lower earnings. One way of allowing for $ATE(X_i)$ to vary with $X_i$ is to assume two different linear regressions for treated and untreated. This is equivalent to specifying one linear regression with an interaction term. 

$$
\mathbb{E}[Y_i | D_i, X_i] = \gamma_0 + \gamma_1 D_i + \gamma_2' X_i + \gamma_3' (D_i X_i).
$$
In this case $ATE(X_i) = \gamma_1 + \gamma_3' X_i$ and $ATE = \gamma_1 + \gamma_3' \mathbb{E}[X_i]$. And easier way to estimate the ATE (under the linearity assumption) is to reparametrize the above specification and run

$$
Y_i = \gamma_0^* + \gamma_1^* D_i + \gamma_2^{*'} X_i + \gamma_3^{*'} (D_i \tilde{X}_i) + \eta_i
$$

where $\tilde{X}_i$ is $X_i$ minus its mean. Then it can be showed that $ATE = \gamma_1^*$.

```{r}
Y <- obs_df$re78
D <- obs_df$treat
X <- dplyr::select(obs_df, c("age", "education", "black", "hispanic", "nodegree"))
X$agesq <- X$age^2
DXc <- D*scale(X, center = TRUE, scale = FALSE)
colnames(DXc) <- paste("dx",names(X), sep = "_")
df3 <- cbind(Y = Y,D,X,DXc)
m3obs <- lm(Y ~., df3)
lmtest::coeftest(m3obs, vcov=sandwich::vcovHC(m3obs, type='HC2'))[2,]
```

## Inverse propensity score weighting (IPW)

When the dimension of $X_i$ is large it is handy to define the propensity score:

$$
p(X_i) = \mathbb{P}(D_i = 1 | X_i),
$$

i.e. the probability of being treated given the covariates. This is useful thanks to two results in Rosenbaum & Rubin(1983). Mainly that if selection on observables holds and $A2: 0< p(X_i) < 1$ a.s. (Common Suppport), then $(Y(1)_i, Y(0)_i) \perp D_i | p(X_i)$, i.e. it is enough to control for $p(X_i)$. Also, if $p(X_i)$ is the propensity score then $X_i \perp D_i | p(X_i)$, so the distribution of $X_i$ does not vary across treatment status given the propensity score. One can show that under A1' and A2

$$
\tau_0 = \mathbb{E}\biggl(\frac{D_i Y_i}{p(X_i)} - \frac{(1-D_i)Y_i}{1 - p(X_i)} \biggr).
$$
However, $p(X_i)$ is generally unknown and has to be estimated. To this end we can impose a parametric model. A popular choice is 

$$
p(X_i) = G(X_i' \beta_0),
$$
where $G(\cdot)$ is the c.d.f of a distribution which is symmetric around 0 and $\beta_0$ is a vector of parameters. Then the conditional likelihood of $D_i|X_i$ is

$$
f(D_i|X_i; \beta) = G(X_i' \beta)^{D_i}[1-G(X_i' \beta)]^{1-D_i}
$$

and the log-likelihood

$$
l_i(\beta) = D_i  \log G(X_i' \beta) + (1 - D_i) \log[1 - G(X_i' \beta)] 
$$
and $\hat{\beta} = \arg \max_\beta \sum_{i=1}^n l_i(\beta)$. A popular choice of $G(\cdot)$ is the logit model

$$
G(z) = \frac{e^z}{1 + e^z},
$$

for which it is simple to compute the score. To estimate the propensity score $\hat{p}(X_i) = G(X_i' \hat{\beta})$ using a logit model in R we can do the following (we introduce as new covariates age squared, married and earnings in 1975 and its squared following [@dehejia1999causal])

```{r}
obs_df$agesq <- obs_df$age^2
obs_df$educationsq <- obs_df$education^2
obs_df$re75sq <- obs_df$re75^2
logm = glm(treat ~ age + agesq + education + educationsq +
           married + re75 + re75sq + black + hispanic + nodegree,
             data = obs_df, family = "binomial")
obs_df$pscore <- logm$fitted.values
b <- as.matrix(logm$coefficients)
```


Now, we are ready to estimate $\tau_0$ as

$$
\hat{\tau}^{IPW}_1 = \frac{1}{n}\sum_{i=1}^n \frac{D_i Y_i}{\hat{p}(X_i)} - \frac{1}{n}\sum_{i=1}^n\frac{(1-D_i)Y_i}{1-\hat{p}(X_i)}
$$
Noting that $\mathbb{E}[D_i/p(X_i)] = 1$ by LIE and the same happens with $\mathbb{E}[(1-D_i)/(1-p(X_i))]$, an asymptotically equivalent estimator is

$$
\hat{\tau}^{IPW}_2 = \biggl(\sum_{i=1}^n \frac{D_i}{\hat{p}(X_i)}\biggr)^{-1} \biggl(\sum_{i=1}^n \frac{D_i Y_i}{\hat{p}(X_i)}\biggr) - \biggl(\sum_{i=1}^n \frac{1-D_i}{1-\hat{p}(X_i)}\biggr)^{-1} \biggl(\sum_{i=1}^n\frac{(1-D_i)Y_i}{1-\hat{p}(X_i)}\biggr)
$$

$\hat{\tau}^{IPW}_1$ and $\hat{\tau}^{IPW}_2$ are called Inverse Probability Weight (IPW) estimators and are a difference of weighted sums of $Y_i$. In $\hat{\tau}^{IPW}_2$ the weights sum up to $1$ and we have better finite sample properties. In fact, both estimators are members of the broader class of consistent, semiparametric estimators found in [@robins1994estimation]. In this paper they find the estimator achieving the least asymptotic variance in this class to be none of the above but

$$
\hat{\tau}^{DR} = \frac{1}{n}\sum_{i=1}^n \frac{D_iY_i - \hat{m}_1(X_i)(D_i - \hat{p}(X_i))}{\hat{p}(X_i)} - \frac{1}{n}\sum_{i=1}^n \frac{(1-D_i)Y_i - \hat{m}_0(X_i)(D_i - \hat{p}(X_i))}{1 - \hat{p}(X_i)}, 
$$

where $m_j(X_i) = \mathbb{E}[Y_i | D_i = j, X_i]$, $j = 0,1$ and $\hat{m}_j(X_i)$ is some estimator. This estimator is called Doubly robust because it has the attractive property that if we impose some parametric specification say $m_j(X_i) = m_j(X_i, \alpha_j)$, the estimator is still consistent if $m_j$ is incorrectly specified but the specification of the propensity score is correct and it is also consistent if the specification of the propensity score is not correct but that of $m_j$ is correct. This estimator is also called augmented IPW since it is "augmented" with the outcome regression. Now we are ready to do these estimations in R

```{r}
Y <- obs_df$re78
D <- obs_df$treat
PS <- obs_df$pscore

ipw1 <- mean(D*Y/PS - (1-D)*Y/(1-PS))
ipw2 <- (1/(sum(D/PS)))*sum(D*Y/PS) - (1/(sum((1-D)/(1-PS))))*sum((1-D)*Y/(1-PS))

# Doubly Robust
m1 <- lm(re78 ~ poly(age,2) + education + black + hispanic + nodegree, filter(obs_df,treat == 1))
m1hat <- predict(m1, obs_df)
m0 <- lm(re78 ~ poly(age,2) + education + black + hispanic + nodegree, filter(obs_df,treat == 0))
m0hat <- predict(m0, obs_df)

ipwdr <- mean((D*Y - (D-PS)*m1hat)/PS) - mean(((1-D)*Y - (D-PS)*m0hat)/(1-PS))
c("IPW1" = ipw1, "IPW2" = ipw2, "DR" = ipwdr)
```

We can see that the estimates are not very stable. This can happen when the common support assumption is violated since having estimated propensity scores in the denominators which are close to zero will produce instability. In our case it is very hard to maintain the common support assumption. To see these let us do an histogram with the propensity scores by treatment status.

```{r}
c1 <- rgb(173,216,230,max = 255, alpha = 180, names = "lt.blue")
c2 <- rgb(255,192,203, max = 255, alpha = 180, names = "lt.pink")

par(mar = c(5, 5, 5, 5) + 0.3)
hist(filter(obs_df, treat == 1)$pscore, freq = TRUE, col = c1, axes = FALSE, xlab = "", ylab = "", main = "")
axis(side = 1, xlim = c(0,1))
axis(side = 4, ylab = "")
mtext(side = 4, text = "NSW", line = 2.5, col = "blue")
par(new=TRUE)
hist(filter(obs_df, treat == 0)$pscore, ylim = c(0,3000), freq = TRUE, axes = FALSE, col = c2, xlab = "", ylab = "", main = "Common Support")
axis(side = 2)
mtext(side = 2, text = "CPS", line = 2.5, col = "pink")
```


We see that the control group (the CPS observations) are highly concentrated close to zero. This is because the NSW experiment chose disadvantaged individuals which are not representative of the US population as the CPS is. Hence, we have a comparability issue. A practical solution is to trim the observations with very low (or high) propensity scores. For instance, if we trim observation with a propensity score lower than 0.1 and higher than 0.9 we see the Common Support assumption seems more plausible

````{r}
obs_dftrim <- filter(obs_df, pscore > 0.1 & pscore < 0.9)

par(mar = c(5, 5, 5, 5) + 0.3)
hist(filter(obs_dftrim, treat == 1)$pscore, freq = TRUE, col = c1, axes = FALSE, xlab = "", ylab = "", main = "")
axis(side = 1, xlim = c(0,1))
axis(side = 4, ylab = "")
mtext(side = 4, text = "NSW", line = 2.5, col = "blue")
par(new=TRUE)
hist(filter(obs_dftrim, treat == 0)$pscore, freq = TRUE, axes = FALSE, col = c2, xlab = "", ylab = "", main = "Common Support")
axis(side = 2)
mtext(side = 2, text = "CPS", line = 2.5, col = "pink")
```


and the estimates become more stable.

```{r, warning=FALSE}
Ytr <- obs_dftrim$re78
Dtr <- obs_dftrim$treat
PStr <- obs_dftrim$pscore

ipw1tr <- mean(Dtr*Ytr/PStr - (1-Dtr)*Ytr/(1-PStr))
ipw2tr <- (1/(sum(Dtr/PStr)))*sum(Dtr*Ytr/PStr) - (1/(sum((1-Dtr)/(1-PStr))))*sum((1-Dtr)*Ytr/(1-PStr))

# Doubly Robust
m1 <- lm(re78 ~ poly(age,2) + education + black + hispanic + nodegree, filter(obs_dftrim,treat == 1))
m1hat <- predict(m1, obs_dftrim)
m0 <- lm(re78 ~ poly(age,2) + education + black + hispanic + nodegree, filter(obs_dftrim,treat == 0))
m0hat <- predict(m0, obs_dftrim)

ipwdrtr <- mean((Dtr*Ytr - (Dtr-PStr)*m1hat)/PStr) - mean(((1-Dtr)*Ytr - (Dtr-PStr)*m0hat)/(1-PStr))
c("IPW1" = ipw1tr, "IPW2" = ipw2tr, "DR" = ipwdrtr)
```

For the rest of this exercise let us restrict the sample to these observations

```{r}
obs_df <- obs_dftrim
PS <- PStr
Y <- Ytr
D <- Dtr
ipw1 <- ipw1tr
ipw2 <- ipw2tr
```

## Inference on IPW

$\hat{\tau}_1^{IPW}$ and $\tau_2^{IPW}$ can be written as Z-estimators. Let us illustrate this for $\hat{\tau}_1^{IPW}$, let $Z_i = (Y_i,D_i,X_i)$ be the data and $\theta_0 = (\tau_0, \beta')'$. Define

$$
q(Z_i, \theta) = \begin{pmatrix}
\frac{D_i Y_i}{G(X_i' \beta)} - \frac{(1-D_i)Y_i}{1 - G(X_i' \beta)} - \tau \\
X_i(D_i - G(X_i'\beta))
\end{pmatrix}
$$
$q(Z_i, \theta)$ is a $(k+1) \times 1$ vector function where $k$ is the dimension of $\beta$. The last $k$ elements of $q(Z_i, \theta)$ are the FOCs of the MLE from the Logit model (the score). Then, $\mathbb{E}[q(Z_i, \theta_0)] = 0$ and $(\hat{\tau}^{IPW}_1, \hat{\beta})$ are chosen such that $\sum_{i=1}^n q(Z_i, \hat{\theta}) = 0$. Let $\dot{q}(Z_i, \theta)$ be the matrix of derivatives of $q$ with respect to parameters in $\theta$. By taking a mean value expansion around $\theta_0$ and rearranging we can show that 

$$
\sqrt{n}(\hat{\theta} - \theta_0) = - \biggl(n^{-1}\sum_{i=1}^n \dot{q}(Z_i, \bar{\theta})\biggr)^{-1} \biggl(n^{-1/2} \sum_{i=1}^n q(Z_i, \theta_0) \biggr) \to_d \mathcal{N}(0,\mathbb{E}[\dot{q}(Z_i,\theta_0)]^{-1} \mathbb{E}[q(Z_i, \theta_0)^2]\mathbb{E}[\dot{q}(Z_i,\theta_0)]^{-1}),
$$

where $\bar{\theta}$ is some intermediate point between $\hat{\theta}$ and $\theta_0$. So we can estimate the variance as

$$
\hat{V}_1 = \biggl(n^{-1}\sum_{i=1}^n \dot{q}(Z_i,\hat{\theta})\biggr)^{-1} \biggl(n^{-1}\sum_{i=1}^nq(Z_i, \hat{\theta}) q(Z_i, \hat{\theta})' \biggr) \biggl(n^{-1}\sum_{i=1}^n \dot{q}(Z_i,\hat{\theta})\biggr)^{-1}
$$


Noticing that $\frac{d}{d u} G(u) = G(u)(1-G(u))$ we can compute the $(k + 1) \times (k +1)$ matrix $\dot{q}$

$$
\dot{q}(Z_i, \theta) = \begin{pmatrix}
-1 & -X_i'\biggl(\frac{D_iY_i(1-G(X_i'\beta))}{G(X_i'\beta)} - \frac{(1-D_i)Y_iG(X_i'\beta)}{1-G(X_i' \beta)}\biggr) \\
0 & X_iX_i'G(X_i' \beta)(1-G(X_i'\beta))
\end{pmatrix}
$$

So we can estimate the standard error of $\hat{\tau}^{IPW}_1$

```{r}
obs_df$const <- rep(1,nrow(obs_df))
X <- as.matrix(dplyr::select(obs_df,c(const,age,agesq,education,educationsq,married,
                               re75,re75sq,black,hispanic,nodegree)))
A <- 0
B12 <- 0
B22 <- 0
G <- function(u){exp(u)/(1+exp(u))}
k <- ncol(X)

for (i in 1:nrow(X)){
  aux <-  matrix(c(D[i]*Y[i]/PS[i] - (1-D[i])*Y[i]/(1-PS[i]) - ipw1,
                    X[i,]*(D[i] - PS[i])),k+1,1)
  A <- A + aux%*%t(aux)
  B12 <- B12 - t(X[i,])*((D[i]*Y[i]*(1-PS[i]))/PS[i] + ((1-D[i])*Y[i]*PS[i])/(1-PS[i]))
  B22 <- B22 - (X[i,] %*% t(X[i,]))*(PS[i]*(1-PS[i]))
}
B <- (1/nrow(X))*rbind(cbind(-nrow(X),B12), cbind(rep(0,k),B22))
V <- inv(B) %*% ((1/nrow(X))*A) %*% inv(B)
se <- sqrt(V[1,1]/nrow(X))
c("IPW1" = ipw1, "SE" = se)
```
Inference for $\hat{\tau}_2^{IPW}$ and $\hat{\tau}^{DR}$ inference follows similarly by framing the problem as a Z-estimator. With a little tweak in the double robust estimator we can get simpler inference.

## Locally Robust IPW

Note that we can rewrite $\hat{\tau}^{DR}$ as

$$
\hat{\tau}^{DR} = \frac{1}{n}\sum_{i=1}^n \biggl( \hat{m}_1(X_i) - \hat{m}_0(X_i) + \frac{D_i}{\hat{p}(X_i)}[Y_i - \hat{m}_1(X_i)] -  \frac{1-D_i}{1 - \hat{p}(X_i)}[Y_i - \hat{m}_0(X_i)] \biggr).
$$

Note that this resembles the direct estimation of the ATE but with an added term, this term is a correction term which ensures local robustness. Now define 

$$
\psi(Z_i, p, m_1, m_0, \tau) = m_1(X_i) - m_0(X_i) + \underbrace{\frac{D_i}{p(X_i)}[Y_i - m_1(X_i)] -  \frac{(1-D_i)}{1 - p(X_i)}[Y_i - m_0(X_i)]}_{\phi(Z_i,p,m_1,m_0)} - \tau,
$$
and notice that $\hat{\tau}^{DR}$ is the $\tau$ which solves $\sum_{i=1}^n \psi(Z_i, \hat{p}, \hat{m}_1, \hat{m}_0, \tau) = 0$. The $\phi$ function is sometimes called the First Step Influence Function (FSIF, see [@chernozhukov2022locally]). As already stated it makes the orthogonal moment function locally robust by capturing the effect of first steps estimation. It turns out that $\psi$ has another very special property: that of an orthogonal moment function. This means that, locally, estimating nuisance functions $p$ $m_1$ and $m_0$ has no impact. In fact we can even estimate them non-parametrically under mild rate conditions. Modifying slightly the estimator to estimate $p$, $m_1$ and $m_0$ with observations different from the ones we use to estimate $\tau_0$ (this is called cross-fitting) allows us to avoid over-fitting, use many non-parametric estimators such as machine learners and provide valid inference based only on the orthogonal moment function $\psi$. Hence, we define the locally robust IPW estimator as a cross-fitted version of the above. Partition the observations in $L$ sets denoted by $I_1,...,I_L$, then 

$$
\hat{\tau}^{LR} = \frac{1}{n} \sum_{l=1}^L \sum_{i \in I_l} \biggl(\hat{m}_{1,l}(X_i) - \hat{m}_{0,l}(X_i) + \frac{D_i}{\hat{p}_l(X_i)}[Y_i - \hat{m}_{1,l}(X_i)] -  \frac{(1-D_i)}{1 - \hat{p}_l(X_i)}[Y_i - \hat{m}_{0,l}(X_i)] \biggr)
$$
To estimate it we split the data in two.

```{r}
set.seed(123)
n <- nrow(obs_df)
L <- 2
ind <- split(seq(n), seq(n) %% L)

fv1 <- rep(0,n)
fv0 <- rep(0,n)
ps <- rep(0,n)
for (i in 1:L){ 
  mps <- ranger(treat ~ age + agesq + education + educationsq + married + re75 + re75sq + black +    
                   hispanic + nodegree, data = obs_df[ind[[i]],])
  mfv1 <- ranger(re78 ~ treat + age + agesq + education + black + hispanic + nodegree,
                data = filter(obs_df[ind[[i]],], treat == 1))
  mfv0 <- ranger(re78 ~ treat + age + agesq + education + black + hispanic + nodegree,
                data = filter(obs_df[ind[[i]],], treat == 0))
  
  fv1[-ind[[i]]] <- predict(mfv1, obs_df[-ind[[i]],])$predictions
  fv0[-ind[[i]]] <- predict(mfv0, obs_df[-ind[[i]],])$predictions
  ps[-ind[[i]]] <- predict(mps, obs_df[-ind[[i]],])$predictions

}

ipwlr_sc <- fv1 - fv0 + (D/ps)*(Y-fv1) - ((1-D)/(1-ps))*(Y-fv0)
ipwlr <- mean(ipwlr_sc)
se_lr <- sd(ipwlr_sc)/sqrt(n)
c(ipwlr = ipwlr, se = se_lr)

```

We still get a negative effect and very large standard errors. In general this data seems to be very ill suited for estimating treatment effects since the controls are wildly different from the treated.

## Packages and other sources

For a book on treatment effects one can read [@angrist2009mostly], for a reference on IPW  is a good reference is [@lunceford2004stratification]. The theory of the logistic regression is that of the conditional MLE and can be found for instance in [@wooldridge2002econometric]. A similar tutorial to this can be found [here](https://bookdown.org/halflearned/tutorial/ate1.html#aipw-estimators). Also, there are several packages with functions to perform the above estimation. An example of such a package is [PSweight](https://arxiv.org/pdf/2010.08893.pdf) in [@zhou2020psweight] where the package is compared to other similar packages.

# References

<div id="refs"></div>


