---
title: "R Econometrics"
author: "Joël Terschuur"
output:
  html_document:
    theme: yeti
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
source("./functions.r")
library(dplyr)
library(latex2exp)
library(ggplot2)
library(sandwich)
library(AER)
```

# R Introduction: Swirl package

Either if it is your first time using R, you have not used it in a long time or you have only seldom used it, I recommend using the Swirl package to learn/refresh the basic notions. Swirl offers a series of interactive tutorials which can be easily followed. First we install and then we load the package. An interactive menu will appear

```{r, eval = FALSE}
install.packages("swirl")
library(swirl)
```

If you follow the instructions you will get to a menu of courses, the more you do the better, however I would definitely do the course "R Programming: The basics of programming in R". Also, in [here](https://github.com/swirldev/swirl_courses#intermediate) you can find more courses. To download a course just do the following:

```{r, eval = FALSE}
install.packages("swirl")
library(swirl)
```

I also find the course named "Getting and Cleaning Data" extremely useful. A final piece of advice is to get used to work with Rstudio projects which help you keep your workflow organized, you can learn about how to use them [here](https://www.youtube.com/watch?v=A52Q50sIQ-I).

# Some Statistics

We are going to use the Encuesta de Condiciones de Vida 2019 (ECV 2019) data to illustrate statistical and econometrics concepts. This data is available at the website of the Instituto Nacional de Estadística (INE) and contains information about
annual income in 2018. This survey is one of the main references for the analysis of the income distribution
and poverty in Spain and it is part of the European survey EU-SILC. We use a cleaned version which is used in [this paper](https://arxiv.org/pdf/2206.05235.pdf) and can be found [here](www.google.com). In the years
2005, 2011 and 2019, the ECV includes a module on intergenerational transmission of disadvantages
with information on circumstances outside the control of the individual such as sex, parental education/occupation,
population of the municipality where they grew, whether they grew up without a mother/father, etc.
We measure income with equivalized household income and the level of observation is the
individual. We restrict the sample to those aged between 25 and 59 years old to focus on the
working age population.

For the moment we are just going to focus on income and forget about the circumstances. 
The first thing to do is to load the data and do some summary statistics

```{r}
ecv <- read.csv("../Data/ecv2019.csv")
summary(ecv$Y)
```

This already gives us some useful information. For instance, the average income in Spain is 17,861€, 
the median is 15,937€ meaning that half of the sample has an income below 15,937 €. We also see that the
observation with lowest income has 0.11€ and the one with the most income has 163,597 €. Finally, the poorest 
25% of the sample has an income below 10,480€ and the richest 25% has an income above 22,769€.
This already gives us an image of the distribution of income in our sample. One of the most useful concepts 
which helps us get a more complete image of the distribution is the histogram

```{r}
hist(ecv$Y)
```

The x-axis is split into bins and the y-axis tells us the frequency (how many observations) are in each bin. Even though
our data will always be discrete it is reasonable to model income as a continuous random variable, meaning it has
a Lebesgue p.d.f. We can estimate this pdf:

```{r}
# Calculate density
dy <- density(ecv$Y)
# Add density
plot(dy, lwd = 2, col = "red", main = "Income pdf", ylab = TeX("$f_n(y)$"))
```

As could already be seen from the histogram, the income distribution is characterized by a fat right tail. This means 
that the probability of extreme positive values does not decay as fast as for instance in the normal distribution. In
economic terms this means that the probability of encountering individuals much richer than the rest does not go down
as fast as in other distributions such as the normal distribution. 

Note that the y-axis of the density does not reflect the frequency or probability of the income values. This is because
the p.d.f. has to integrate to 1, $\int f(y) dy = 1$. We can plot the histogram and the density together if we force the 
histogram to integrate to 1 as well.


```{r}
# Create a histogram
hist(ecv$Y, freq = FALSE, main = "Histogram and density", ylim = c(0, max(dy$y)))
# Add density
lines(dy, lwd = 2, col = "red")
```

Another key statistical concept is the c.d.f., $F(x)$. Its sample analog is the empirical distribution function
e.c.d.f. $F_n(y) = n^{-1}\sum_{i=1}^n 1(Y_i \leq y)$. This we can also compute and visualize with R.

```{r}
# Let n be the number of obs
n <- length(ecv$Y)
#Create step function object with sorted Y in y axis and the
#accumulation of 1/n in the x-axis
step_ecdf <- stepfun(sort(ecv$Y),c(0,(1:n)/n))
#Plot e.c.d.f
plot(step_ecdf, main = "ECDF", ylab = TeX("$F_n(y)$"))
```

Of course, the e.c.d.f. satisfies all the properties of a c.d.f. Since
we have a lot of observations the e.c.d.f looks almost continuous, however it is a step function. If we do the same with a random subset of only 20 observations

```{r}
#Create random sample
Yr <- ecv$Y[sample(1:n, 20, replace = TRUE)]
#Create step function object
step_ecdf20 <- stepfun(sort(Yr),c(0,(1:20)/20))
#Plot e.c.d.f
plot(step_ecdf20, main = "ECDF n = 20", ylab = TeX("$F_n(y)$"))
```

Quantiles are another key tool to understand the distribution of income, for $\tau \in [0,1]$

$$ F^{-1}(\tau) = inf\{y: F(y) \geq \tau \} $$
this is also called the generalized inverse. If $F$ is continuous (which implies monotone increasing since its 
a cumulative function) then $F^{-1}(\tau)$ is the usual inverse. To find the quantile function in our data we
can just look at the inverse of our e.c.d.f.

```{r}
ecdf = (1:n)/n
df_q <- as_tibble(cbind(Y = sort(ecv$Y), ecdf = ecdf))
plot(stepfun(ecdf,c(sort(ecv$Y),max(ecv$Y))),
     xlim = c(0,1),
     main = "Quantile function",
     ylab = TeX("$F_n^{-1} (\tau)$"))
```

Popular inequality measures are quantile share ratios, for instance the quintile share ratios $S80/S20$, $S80/S40$
and $S80/S60$ compute the ratio of the total income received by the top 20% of the population against the total income
received by the poorest 20%, 40% and 60% respectively. Let's start using ggplot now

```{r}
quint_grid <- c(0.2,0.4,0.6)
s80 <- sum(df_q$Y[which.min(abs(ecdf - 0.8)):n])
s_quint <- vapply(quint_grid,
                  function (x)  sum(df_q$Y[1:which.min(abs(ecdf - x))]),
                  1)
s80_quints <- as_tibble(cbind(grid = quint_grid,
                              ratio = s80/s_quint))

ggplot(s80_quints, aes(grid,ratio)) +
  geom_point(color = "red", size = 3) + 
  labs(x = "X", y = "S80/SX", title = "Share Quintile ratios")  
  
```

So we can see that the income share of the top 20% is more than 6 times that of the poorest 20%.
We can also look at the share of the top 10% against the share of the other deciles

```{r}
dec_grid <- seq(0.1,0.8,0.1)
s90 <- sum(df_q$Y[which.min(abs(ecdf - 0.9)):n])
s_dec <- vapply(dec_grid,
                  function (x)  sum(df_q$Y[1:which.min(abs(ecdf - x))]),
                  1)
s90_decs <- as_tibble(cbind(grid = dec_grid,
                              ratio = s90/s_dec))

ggplot(s90_decs, aes(grid,ratio)) +
  geom_point(color = "red", size = 3) + 
  labs(x = "X", y = "S90/SX", title = "Share Decile ratios")  
  
```

Here we can see that the income share of the top 10% is more than 10 times that of the poorest 10%.
Finally, we can also compare the top 5% with poorest 5%, 10%, 15%, etc.

```{r}
f_grid <- seq(0.05,0.9,0.05)
s95 <- sum(df_q$Y[which.min(abs(ecdf - 0.95)):n])
s_f <- vapply(f_grid,
                  function(x)  sum(df_q$Y[1:which.min(abs(ecdf - x))]),
                  1)
s95_fs <- as_tibble(cbind(grid = f_grid,
                              ratio = s95/s_f))

ggplot(s95_fs, aes(grid,ratio)) +
  geom_point(color = "red", size = 3) + 
  labs(x = "X", y = "S95/SX", title = " 95% Share ratios")  
  
```

So the share of the top 5% is almost 20 times larger than the share of the bottom 5%.
A fundamental tool in inequality is the Lorenz curve, this curve tracks the share of total
income that each percentile has, formally 

$$
L(p) = \mu^{-1} \int_0^p F^{-1}(u) \, du
$$
Using the empirical quantile function we can plot this Lorenz curve

```{r}
shares <- vapply(ecdf,
                  function(x)  sum(df_q$Y[1:which.min(abs(ecdf - x))])/sum(df_q$Y),
                  1)

lorenz <- as_tibble(cbind(p = ecdf,
                          shares = shares))

ggplot(lorenz, aes(p,shares)) +
  geom_line(color = "red") + 
  geom_abline(intercept = 0, slope = 1) +
  labs(x = "p", y = "L(p)", title = "Lorenz curve")  
  
```

Suppose that there was one individual who own all the income, then the Lorenz curve would be equal to $0$ and jump
to one for $p=1$. If income was equally distributed the Lorenz curve would be equal to the $45^o$ degrees line.
This motivates the use of the area between the $45^o$ degrees line and the Lorenz curve as a measure of inequality.
If $A$ is the area between the $45^o$ degrees line and the Lorenz curve and $B$ is the area below the Lorenz curve,
the Gini coefficient is

$$
G = \frac{A}{A+B} = 1 - 2 \int_0^1 L(p) \, dp
$$
Again, using the empirical counterparts we have that

```{r}
#numerical integral
dp <- c(0,lorenz$p)
aux <- 0
n1 <- n-1
for (i in 1:n1){
  aux <- aux + lorenz$shares[i]*(dp[i+1] - dp[i])
}
G = 1 - 2*aux
print(paste0("The Gini coefficient is ", round(G,2)))
```
# LLN and CLT

In this part we are going to illustrate the Law of the Large Numbers and the Central Limit Theorem. For this we are
going to forget about the data and simulate our own income data. A common parametric model for income is the log-normal
model

$$
Y_i = e^{X_i}, \text{ with } X_i \sim \mathcal{N}(0,\sigma^2).
$$
This model is called log-normal because once you take logs the model is normal: $\ln Y_i = X_i$. It is particularly
useful for modelling variables with fat right tails such as income. Let's generate $3000$ observations for
$(\mu,\sigma) = (0,1)$ and plot an estimate of the density


```{r}
n <- 3000
mu <- 0
s <- 1
X <- rnorm(n,mu,s)
Y <- exp(X)
dy <- density(Y)
# Add density
plot(dy, lwd = 2, col = "red", main = "Simulated Y", ylab = TeX("$f_n(y)$"))
```


As we can see the distribution does seem to have a fat tail. A nice thing about simulations is that you 
know the true value of the parameters, in this case we have a log-normal distribution with parameters
$(\mu,\sigma) = (0,1)$. Given these parameters we can compute the true expectation of $Y$

$$
\mathbb{E}(Y_i) = e^{\mu + \sigma^2/2} = e^{1/2} = 1.648721...
$$

Let $\bar{Y} = n^{-1} \sum_{i=1}^n Y_i$, the LLN tells us that for all $\varepsilon > 0$

$$
\lim_{n \to \infty} \mathbb{P} \biggl( |\bar{Y} - \mathbb{E}(Y_i)| > \varepsilon \biggr) = 0,
$$

i.e. $\bar{Y} \to_p \mathbb{E}(Y_i)$. We can compute the sample average for different sample sizes to check whether
this is true

```{r}
ngrid <- seq(10,10000,10)
EY <- exp(0.5)
lln <- vapply(ngrid, function(x){
  X <- rnorm(x,0,1)
  Y <- exp(X)
  mY <- mean(Y)},1)
lln <- as_tibble(cbind(n = ngrid, mean = lln))
ggplot(lln, aes(n,mean)) + 
  geom_line(color = "red") + 
  geom_abline(intercept = EY, slope = 0) + 
  labs(title = "LLN")
```

So we see that for increasing sample sizes we concentrate around the true value. How much it takes to get to concentrate
around the true value can change a lot depending on the underlying distribution, the LLN is an asymptotic result so it
tells us what happens in the limit not on the way to the limit. For instance, let's see what happens if we compare our
simulated data with normally generated data with the same mean and variance equal to $1/2$.

```{r}

lln2 <- vapply(ngrid, function(x) mean(rnorm(x,EY,0.5)),1)
lln2 <- as_tibble(cbind(n = ngrid, mean = lln2, distr = rep(2,length(ngrid))))
lln1 <- as_tibble(cbind(lln, distr = rep(1,length(ngrid))))
lln_distr <- rbind(lln1,lln2)

ggplot(lln_distr, aes(n,mean)) +
  geom_line(aes(col = factor(distr))) +
  geom_abline(intercept = EY, slope = 0) +
  scale_color_discrete(name = "Distribution", labels = c("LN","N")) + 
  labs(title = "LLN")
```

We see that the sample mean when the DGP is $\mathcal{N(e^{1/2},1/2)}$ concentrates much faster around the true value.

The CLT states that

$$
\sqrt{n}(\bar{Y} - \mathbb{E}(Y_i)) \to_d \mathcal{N}(0,\mathbb{V}ar(Y_i))
$$

This means that for large $n$, $\bar{Y}$ is approximately distributed as a $\mathcal{N}(\mathbb{E}(Y_i), \mathbb{V}ar(Y_i)/n)$.
We call $\sqrt{\mathbb{V}ar(Y_i)/n}$ the standard error of $\bar{X}$. For the log-normal we have that 
$\mathbb{V}ar(Y_i) = (e^{\sigma^2} - 1)e^{2\mu + \sigma^2}$.
To see how this approximation works we can extract many random samples, say $R$, of same sample size $n$
and compute the sample mean for each one of them. That is, in the end we end up with a vector with $R$ sample means.
This can be seen as $R$ random draws from the distribution of $\bar{Y}$ and hence they can be used to see how good
the approximation is. If we do this for different sample sizes we can see how the CLT kicks in.


```{r}
ngrid <- c("n100" = 100,"n3000" = 3000)
R <- 10000
Rgrid <- 1:R
mu <- 0
s <- 1

MC <- NULL
for (i in 1:length(ngrid)){
  MC <- cbind(vapply(Rgrid, function(x){
  X <- rnorm(x,0,1)
  Y <- exp(X)
  mY <- mean(Y)},1),MC)
}
colnames(MC) <- names(ngrid)
MC <- as_tibble(MC)
se_true <- sqrt(((exp(s^2) - 1)*(exp(2*mu) + s^2))/ngrid)

c1 <- rgb(0,255,255,max = 255, alpha = 150, names = "lt.blue")
xx <- c(1.5,1.8)
yy <- c(0,15)
br <- 300
lsz <- 0.5
lpos <- "topright"

par(mfrow = c(1,2)) 

hist(MC$n100, breaks = br, xlim = xx, ylim = yy,
     xlab = "", ylab = "", freq = FALSE, col = c1, main = "")
curve(dnorm(x,mean = EY, sd = se_true[1]),
      col = "red", lwd = 2, add = TRUE)
      title(main = "n = 100")
      legend(lpos, legend = c("Simulation","Normal Distr."),
      cex = lsz, col = c(c1,"red"), pch = c(15,NA), lty = c(NA,1))
      abline(v=EY,col="red",lwd=2)


hist(MC$n3000, breaks = br, xlim = xx, ylim = yy,
     xlab = "", ylab = "", freq = FALSE, col = c1, main = "")
curve(dnorm(x,mean = EY, sd = se_true[2]),
      col = "red", lwd = 2, add = TRUE)
abline(v=EY,col="red",lwd=2)
title(main = "n = 3000")
legend(lpos, legend = c("Simulation","Normal Distr."),
       cex = lsz, col = c(c1,"red"), pch = c(15,NA), lty = c(NA,1))

```

We see that as theory predicts the sample mean is unbiased and the normal approximation improves as $n$ grows.

# Ordinary Least Squares

We can go back to our ECV data to run some regressions. To run a regression of log income on sex we write

```{r}
m1 <- lm(log(Y) ~ sex, ecv) 
summary(m1)
```

So we see that the expected wage for males is 3% higher than that of female. However, our standard errors
are not robust to heteroskedasticity. Unfortunately, in R it is a bit more of a hassle to have
robust standard errors than in Stata, still it is not very hard either if we use the library $\texttt{sandwich}$ and
$\texttt{AER}$

```{r}
vcv <- vcovHC(m1, type = "HC1")
coeftest(m1, vcv)
```
So we see the coefficient is significant 5% level. A good exercise is to not use any package. Let $X_i = (1,sex_i)'$, we
know that the ols estimator is

$$
\hat{\beta} = \mathbb{E}_n(X_i X_i')^{-1}\mathbb{E}_n(X_i Y_i) = (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbb{Y},
$$

where $\mathbb{X} = (X_1',...,X_n')$ is called the design or model matrix and in this case has two columns, the first one
filled with ones and the second one is $(sex_1,...,sex_n)'$ and $\mathbb{Y}$ is a column with $(Y_1,...,Y_n)$. We can write our own function s_reg and use it instead of the inbuilt regression package. For now just look at the function s_reg up to where bhat is created, the rest is for the standard errors which will be explained right after.

```{r}
n <- length(ecv$Y)
Y <- log(ecv$Y)
sex <- as.numeric(as.factor(ecv$sex)) - 1
XX <- cbind(rep(1,n),sex)
#number of regressors (including constant)
k <- ncol(XX)

s_reg <- function(X,Y, se = NULL){
  n <- length(Y)
  Y <- as.matrix(Y)
  bhat <- solve(t(X)%*%X)%*%(t(X)%*%Y)
  u <- Y - X%*%bhat
  if (is.null(se) == 1){
    Shat <- solve((1/n)*(t(X) %*% X))*as.numeric((1/(n-k))*(t(u)%*%(u)))
  }
  else{
    B <- 0
    B <- (t(as.matrix(X)*as.numeric(u)))%*%(as.matrix(X)*as.numeric(u))
    Shat <- (solve((1/n)*(t(X) %*% X)) %*% ((1/(n-k)*B)) %*% solve((1/n)*(t(X) %*% X)))
  }
  sehat <- sqrt(Shat[row(Shat) == col(Shat)]/n)
  res <- list("b" = bhat, "se" = sehat)
}

m1 <- s_reg(XX,Y)
print(m1$b)

```

As you see we get exactly the same as with the $\texttt{lm()}$ function. From OLS theory we know that under homoskedasticity

$$
\sqrt{n}(\hat{\beta} - \beta_0) \to_d \mathcal{N}(0, \Sigma), \text{ } \Sigma = \mathbb{E}(X_i X_i')^{-1} \mathbb{E}(u_i^2)
$$
where $u_i = Y_i - X_i' \beta_0$ and $\beta_0 = \mathbb{E}(X_i X_i')^{-1}\mathbb{E}(X_i Y_i)$, an estimator of $\Sigma$ is

$$
\hat{\Sigma} =  \mathbb{E}_n(X_i X_i')^{-1} \mathbb{E}_n(\hat{u}_i^2) = (\mathbb{X}'\mathbb{X})^{-1} (\hat{\mathbb{u}}'\hat{\mathbb{u}})
$$

where $\hat{\mathbb{u}} = (\hat{u}_1,...,\hat{u}_n)'$ and the standard errors are $se = (\sqrt{\Sigma_{11}/n},\sqrt{\Sigma_{22}/n})$
with estimator $\hat{se} = (\sqrt{\hat{\Sigma}_{11}/n},\sqrt{\hat{\Sigma}_{22}/n})$

```{r}
print(m1$se)
```

More generally (without assuming homoskedasticity) we have 

$$
\sqrt{n}(\hat{\beta} - \beta_0) \to_d \mathcal{N}(0, \Sigma), \text{ } \Sigma = \mathbb{E}(X_i X_i')^{-1} \mathbb{E}(X_i X_i' u_i^2) \mathbb{E}(X_i X_i')^{-1}
$$


and

$$
\hat{\Sigma} = \mathbb{E}_n(X_i X_i')^{-1} \mathbb{E}_n(X_i X_i' \hat{u}_i^2) \mathbb{E}_n(X_i X_i')^{-1} = 
\biggl(\frac{1}{n}\mathbb{X}'\mathbb{X}\biggr)^{-1}\biggl(\frac{1}{n}\sum_{i=1}^n u_i^2 X_i X_i'\biggr)\biggl(\frac{1}{n}\mathbb{X}'\mathbb{X}\biggr)^{-1}
$$

For the middle term the matrix form would be $\mathbb{X}'B\mathbb{X}$ where $B = diag(u_1^2,...,u_n^2)$, since this is an nxn matrix 
it will be heavy on the memory of the computer so it is better to just compute the sum with a loop or (even faster) using element by element multiplication: $(1/n)*(\mathbb{X}**u)'*(\mathbb{X}**u)$, where $**$ stands for element by element multiplication (common multiplication in R). Note that in the function s_reg whenever we compute a mean involving residuals we divide by $n-k$ to correct for the degrees of freedom taken by the estimation of the coefficients, however this is asymptotically negligible.


```{r}
m2 <- s_reg(XX,Y,se = "r")
print(m2$b)
print(m2$se)

```

## Boostrapping standard errors

A common way to get standard errors when no explicit expression is available is to use bootstrap. To illustrate let us compute the standard errors of the above regression with bootstrap (even though we do have an explicit expression for them). The key idea is 
to consider the empirical distribution of our data $F_n$ as the "true" distribution and then we can 
take samples from it and get an estimate for each sample. In this way we can approximate the distribution
of our estimator.

```{r}
B <- 1000

b_boot <- sapply(1:B, function(x){
  ecv_boot <- sample(1:n, n, replace = TRUE)
  ecv_boot <- ecv[ecv_boot,]
  Y <- log(ecv_boot$Y)
  sex <- as.numeric(as.factor(ecv_boot$sex)) - 1
  XX <- cbind(rep(1,n),sex)

  m <- s_reg(XX,Y,se = "r")
  return(m$b)})

print(c(sd(b_boot[1,]),sd(b_boot[2,])))

```


# Bias and Coverage rate in Monte Carlo experiments

When an estimator is proposed together with its inferential theory it is common to do a Monte Carlo (MC)
experiment showing the bias and the coverage rate of the estimators. Let us do a MC to see the bias
of OLS and the coverage rate of different standard error estimators. In the MC we take many samples
from the true DGP and for each one we estimate the coefficients and the standard errors. The bias
will be the difference between the mean of all the estimates and the true value. The coverage rate
is the proportion of times that the true value falls inside the confidence interval or, equivalently,
the proportion of times that we do not reject that the coefficient is equal to the true value.

Let us use the following DGP with heteroskedasticity

$$
\begin{align*}
Y_i &= 1 + 3*X_i + u_i \\
u_i &=  \varepsilon_i*X_i \\
X_i, \varepsilon_i &\sim \mathcal{N}(0,1) , \quad X_i \perp \varepsilon_i
\end{align*}
$$
So now $\mathbb{E}[u_i^2|X_i] = \mathbb{E}[\varepsilon^2]*X_i^2 = X_i^2$, so we have heteroskedasticity and note that $\mathbb{E}[X_i*u_i] = \mathbb{E}[X_i*X_i]*\mathbb{E}[\varepsilon_i] = 0$ so we do not have endogeneity.

In the simulation we report the bias, MSE and coverage for the (incorrect) homoskedastic standard errors, the heteroskedasticity robust standard errors and the bootstrap standard errors. 

```{r}
ngrid <- c(100,500,1000)
R <- 1000
B <- 100
Rgrid <- 1:R
btr <- 0.2
set.seed(123)

MC <- NULL
bias <- rep(0,length(ngrid))
mse <- rep(0,length(ngrid))
cov1 <- rep(0,length(ngrid))
cov2 <- rep(0,length(ngrid))
cov3 <- rep(0,length(ngrid))
for (i in 1:length(ngrid)){
  n <- ngrid[i]
  MC <- lapply(Rgrid, function(x){
    X <- rnorm(n)
    XX <- cbind(rep(1,n),X)
    u <- rnorm(n)*X
    Y <- 1 + btr*X + u
    # dff <- as_tibble(cbind(Y = Y, X = X))
    #Point estimate
    m1 <- s_reg(XX,Y)
    b1 <- m1$b[2]
    # b1 <- lm(Y ~ X, dff)
    # b1 <- b1$coefficients[2]
    #Standard errors
    #non heteroskedasticity robust
    se1 <- m1$se[2]
    rej1 <- (abs(b1 - btr)/se1) >= qnorm(0.975) 
    #heteroskedasticity robust
    m2 <- s_reg(XX,Y, se = "r")
    se2 <- m2$se[2]
    rej2 <- (abs(b1 - btr)/se2) >= qnorm(0.975) 
    #Bootstrap
    b_boot <- sapply(1:B, function(x){
      boot <- sample(1:n, n, replace = TRUE)
      boot <- as_tibble(cbind(Y = Y, X = X)[boot,])
      Yb <- boot$Y
      Xb <- boot$X
      XXb <- cbind(rep(1,n),Xb)
      m <- s_reg(XXb,Yb)
      return(m$b[2])})
    se3 <- sd(b_boot)
    rej3 <- (abs(b1 - btr)/se3) >= qnorm(0.975) 
    return(c(b1,rej1,rej2,rej3))})
  MC <- matrix(unlist(MC),4,R)
  bias[i] <- mean(MC[1,]) - btr
  mse[i] <- (mean(MC[1,]) - btr)^2 + var(MC[1,])
  cov1[i] <- 1 - mean(MC[2,])
  cov2[i] <- 1 - mean(MC[3,])
  cov3[i] <- 1 - mean(MC[4,])
}
res <- cbind(Bias = bias, MSE = mse, Homoskedastic = cov1,
             Heteroskedastic = cov2, Bootstrap = cov3)
row.names(res) <- paste("n = ",ngrid)
res


```

As predicted by OLS theory, OLS is unbiased, its accuracy measured by the MSE decreases as n increases. Also, the coverage of the heteroskedasticity robust standard errors is close to the nominal 0.95 while the standard errors computed under the assumption of homoskedasticity are far from 0.95.


