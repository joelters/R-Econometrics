---
title: "R Econometrics"
author: "Joël Terschuur"
output:
  html_document:
    theme: yeti
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
library(dplyr)
library(latex2exp)
library(ggplot2)
library(sandwich)
library(AER)
```

# R Introduction: Swirl package

Either if it is your first time using R, you have not used it in a long time or you have only seldom used it, I recommend using the Swirl package to learn/refresh the basic notions. Swirl offers a series of interactive tutorials which can be easily followed. First we install and then we load the package. An interactive menu will appear

```{r, eval = FALSE}
install.packages("swirl")
library(swirl)
```

If you follow the instructions you will get to a menu of courses, the more you do the better, however I would definitely do the course "R Programming: The basics of programming in R". Also, in [here](https://github.com/swirldev/swirl_courses#intermediate) you can find more courses. To download a course just do the following:

```{r, eval = FALSE}
install.packages("swirl")
library(swirl)
```

I also find the course named "Getting and Cleaning Data" extremely useful. A final piece of advice is to get used to work with Rstudio projects which help you keep your workflow organized, you can learn about how to use them [here](https://www.youtube.com/watch?v=A52Q50sIQ-I).

# Some Statistics

We are going to use the Encuesta de Condiciones de Vida 2019 (ECV 2019) data to illustrate statistical and econometrics concepts. This data is available at the website of the Instituto Nacional de Estadística (INE) and contains information about
annual income in 2018. This survey is one of the main references for the analysis of the income distribution
and poverty in Spain and it is part of the European survey EU-SILC. We use a cleaned version which is used in [this paper](https://arxiv.org/pdf/2206.05235.pdf) and can be found [here](https://github.com/joelters/home/raw/gh-pages/assets/ecv2019.zip). In the years
2005, 2011 and 2019, the ECV includes a module on intergenerational transmission of disadvantages
with information on circumstances outside the control of the individual such as sex, parental education/occupation,
population of the municipality where they grew, whether they grew up without a mother/father, etc.
We measure income with equivalized household income and the level of observation is the
individual. We restrict the sample to those aged between 25 and 59 years old to focus on the
working age population.

For the moment we are just going to focus on income and forget about the circumstances. 
The first thing to do is to load the data and do some summary statistics

```{r}
ecv <- read.csv("../Data/ecv2019.csv")
summary(ecv$Y)
```

This already gives us some useful information. For instance, the average income in Spain is 17,861€, 
the median is 15,937€ meaning that half of the sample has an income below 15,937 €. We also see that the
observation with lowest income has 0.11€ and the one with the most income has 163,597 €. Finally, the poorest 
25% of the sample has an income below 10,480€ and the richest 25% has an income above 22,769€.
This already gives us an image of the distribution of income in our sample. One of the most useful concepts 
which helps us get a more complete image of the distribution is the histogram

```{r}
hist(ecv$Y)
```

The x-axis is split into bins and the y-axis tells us the frequency (how many observations) are in each bin. Even though
our data will always be discrete it is reasonable to model income as a continuous random variable, meaning it has
a Lebesgue p.d.f. We can estimate this pdf:

```{r}
# Calculate density
dy <- density(ecv$Y)
# Add density
plot(dy, lwd = 2, col = "red", main = "Income pdf", ylab = TeX("$f_n(y)$"))
```

As could already be seen from the histogram, the income distribution is characterized by a fat right tail. This means 
that the probability of extreme positive values does not decay as fast as for instance in the normal distribution. In
economic terms this means that the probability of encountering individuals much richer than the rest does not go down
as fast as in other distributions such as the normal distribution. 

Note that the y-axis of the density does not reflect the frequency or probability of the income values. This is because
the p.d.f. has to integrate to 1, $\int f(y) dy = 1$. We can plot the histogram and the density together if we force the 
histogram to integrate to 1 as well.


```{r}
# Create a histogram
hist(ecv$Y, freq = FALSE, main = "Histogram and density", ylim = c(0, max(dy$y)))
# Add density
lines(dy, lwd = 2, col = "red")
```

Another key statistical concept is the c.d.f., $F(x)$. Its sample analog is the empirical distribution function
e.c.d.f. $F_n(y) = n^{-1}\sum_{i=1}^n 1(Y_i \leq y)$. This we can also compute and visualize with R.

```{r}
# Let n be the number of obs
n <- length(ecv$Y)
#Create step function object with sorted Y in y axis and the
#accumulation of 1/n in the x-axis
step_ecdf <- stepfun(sort(ecv$Y),c(0,(1:n)/n))
#Plot e.c.d.f
plot(step_ecdf, main = "ECDF", ylab = TeX("$F_n(y)$"))
```

Of course, the e.c.d.f. satisfies all the properties of a c.d.f. Since
we have a lot of observations the e.c.d.f looks almost continuous, however it is a step function. If we do the same with a random subset of only 20 observations

```{r}
#Create random sample
Yr <- ecv$Y[sample(1:n, 20, replace = TRUE)]
#Create step function object
step_ecdf20 <- stepfun(sort(Yr),c(0,(1:20)/20))
#Plot e.c.d.f
plot(step_ecdf20, main = "ECDF n = 20", ylab = TeX("$F_n(y)$"))
```

Quantiles are another key tool to understand the distribution of income, for $\tau \in [0,1]$

$$ F^{-1}(\tau) = inf\{y: F(y) \geq \tau \} $$
this is also called the generalized inverse. If $F$ is continuous (which implies monotone increasing since its 
a cumulative function) then $F^{-1}(\tau)$ is the usual inverse. To find the quantile function in our data we
can just look at the inverse of our e.c.d.f.

```{r}
ecdf = (1:n)/n
df_q <- as_tibble(cbind(Y = sort(ecv$Y), ecdf = ecdf))
plot(stepfun(ecdf,c(sort(ecv$Y),max(ecv$Y))),
     xlim = c(0,1),
     main = "Quantile function",
     ylab = TeX("$F_n^{-1} (\tau)$"))
```

Popular inequality measures are quantile share ratios, for instance the quintile share ratios $S80/S20$, $S80/S40$
and $S80/S60$ compute the ratio of the total income received by the top 20% of the population against the total income
received by the poorest 20%, 40% and 60% respectively. Let's start using ggplot now

```{r}
quint_grid <- c(0.2,0.4,0.6)
s80 <- sum(df_q$Y[which.min(abs(ecdf - 0.8)):n])
s_quint <- vapply(quint_grid,
                  function (x)  sum(df_q$Y[1:which.min(abs(ecdf - x))]),
                  1)
s80_quints <- as_tibble(cbind(grid = quint_grid,
                              ratio = s80/s_quint))

ggplot(s80_quints, aes(grid,ratio)) +
  geom_point(color = "red", size = 3) + 
  labs(x = "X", y = "S80/SX", title = "Share Quintile ratios")  
  
```

So we can see that the income share of the top 20% is more than 6 times that of the poorest 20%.
We can also look at the share of the top 10% against the share of the other deciles

```{r}
dec_grid <- seq(0.1,0.8,0.1)
s90 <- sum(df_q$Y[which.min(abs(ecdf - 0.9)):n])
s_dec <- vapply(dec_grid,
                  function (x)  sum(df_q$Y[1:which.min(abs(ecdf - x))]),
                  1)
s90_decs <- as_tibble(cbind(grid = dec_grid,
                              ratio = s90/s_dec))

ggplot(s90_decs, aes(grid,ratio)) +
  geom_point(color = "red", size = 3) + 
  labs(x = "X", y = "S90/SX", title = "Share Decile ratios")  
  
```

Here we can see that the income share of the top 10% is more than 10 times that of the poorest 10%.
Finally, we can also compare the top 5% with poorest 5%, 10%, 15%, etc.

```{r}
f_grid <- seq(0.05,0.9,0.05)
s95 <- sum(df_q$Y[which.min(abs(ecdf - 0.95)):n])
s_f <- vapply(f_grid,
                  function(x)  sum(df_q$Y[1:which.min(abs(ecdf - x))]),
                  1)
s95_fs <- as_tibble(cbind(grid = f_grid,
                              ratio = s95/s_f))

ggplot(s95_fs, aes(grid,ratio)) +
  geom_point(color = "red", size = 3) + 
  labs(x = "X", y = "S95/SX", title = " 95% Share ratios")  
  
```

So the share of the top 5% is almost 20 times larger than the share of the bottom 5%.
A fundamental tool in inequality is the Lorenz curve, this curve tracks the share of total
income that each percentile has, formally 

$$
L(p) = \mu^{-1} \int_0^p F^{-1}(u) \, du
$$
Using the empirical quantile function we can plot this Lorenz curve

```{r}
shares <- vapply(ecdf,
                  function(x)  sum(df_q$Y[1:which.min(abs(ecdf - x))])/sum(df_q$Y),
                  1)

lorenz <- as_tibble(cbind(p = ecdf,
                          shares = shares))

ggplot(lorenz, aes(p,shares)) +
  geom_line(color = "red") + 
  geom_abline(intercept = 0, slope = 1) +
  labs(x = "p", y = "L(p)", title = "Lorenz curve")  
  
```

Suppose that there was one individual who own all the income, then the Lorenz curve would be equal to $0$ and jump
to one for $p=1$. If income was equally distributed the Lorenz curve would be equal to the $45^o$ degrees line.
This motivates the use of the area between the $45^o$ degrees line and the Lorenz curve as a measure of inequality.
If $A$ is the area between the $45^o$ degrees line and the Lorenz curve and $B$ is the area below the Lorenz curve,
the Gini coefficient is

$$
G = \frac{A}{A+B} = 1 - 2 \int_0^1 L(p) \, dp
$$
Again, using the empirical counterparts we have that

```{r}
#numerical integral
dp <- c(0,lorenz$p)
aux <- 0
n1 <- n-1
for (i in 1:n1){
  aux <- aux + lorenz$shares[i]*(dp[i+1] - dp[i])
}
G = 1 - 2*aux
print(paste0("The Gini coefficient is ", round(G,2)))
```
# LLN and CLT

In this part we are going to illustrate the Law of the Large Numbers and the Central Limit Theorem. For this we are
going to forget about the data and simulate our own income data. A common parametric model for income is the log-normal
model

$$
Y_i = e^{X_i}, \text{ with } X_i \sim \mathcal{N}(0,\sigma^2).
$$
This model is called log-normal because once you take logs the model is normal: $\ln Y_i = X_i$. It is particularly
useful for modelling variables with fat right tails such as income. Let's generate $3000$ observations for
$(\mu,\sigma) = (0,1)$ and plot an estimate of the density


```{r}
n <- 3000
mu <- 0
s <- 1
X <- rnorm(n,mu,s)
Y <- exp(X)
dy <- density(Y)
# Add density
plot(dy, lwd = 2, col = "red", main = "Simulated Y", ylab = TeX("$f_n(y)$"))
```


As we can see the distribution does seem to have a fat tail. A nice thing about simulations is that you 
know the true value of the parameters, in this case we have a log-normal distribution with parameters
$(\mu,\sigma) = (0,1)$. Given these parameters we can compute the true expectation of $Y$

$$
\mathbb{E}(Y_i) = e^{\mu + \sigma^2/2} = e^{1/2} = 1.648721...
$$

Let $\bar{Y} = n^{-1} \sum_{i=1}^n Y_i$, the LLN tells us that for all $\varepsilon > 0$

$$
\lim_{n \to \infty} \mathbb{P} \biggl( |\bar{Y} - \mathbb{E}(Y_i)| > \varepsilon \biggr) = 0,
$$

i.e. $\bar{Y} \to_p \mathbb{E}(Y_i)$. We can compute the sample average for different sample sizes to check whether
this is true

```{r}
ngrid <- seq(10,10000,10)
EY <- exp(0.5)
lln <- vapply(ngrid, function(x){
  X <- rnorm(x,0,1)
  Y <- exp(X)
  mY <- mean(Y)},1)
lln <- as_tibble(cbind(n = ngrid, mean = lln))
ggplot(lln, aes(n,mean)) + 
  geom_line(color = "red") + 
  geom_abline(intercept = EY, slope = 0) + 
  labs(title = "LLN")
```

So we see that for increasing sample sizes we concentrate around the true value. How much it takes to get to concentrate
around the true value can change a lot depending on the underlying distribution, the LLN is an asymptotic result so it
tells us what happens in the limit not on the way to the limit. For instance, let's see what happens if we compare our
simulated data with normally generated data with the same mean and variance equal to $1/2$.

```{r}

lln2 <- vapply(ngrid, function(x) mean(rnorm(x,EY,0.5)),1)
lln2 <- as_tibble(cbind(n = ngrid, mean = lln2, distr = rep(2,length(ngrid))))
lln1 <- as_tibble(cbind(lln, distr = rep(1,length(ngrid))))
lln_distr <- rbind(lln1,lln2)

ggplot(lln_distr, aes(n,mean)) +
  geom_line(aes(col = factor(distr))) +
  geom_abline(intercept = EY, slope = 0) +
  scale_color_discrete(name = "Distribution", labels = c("LN","N")) + 
  labs(title = "LLN")
```

We see that the sample mean when the DGP is $\mathcal{N(e^{1/2},1/2)}$ concentrates much faster around the true value.

The CLT states that

$$
\sqrt{n}(\bar{Y} - \mathbb{E}(Y_i)) \to_d \mathcal{N}(0,\mathbb{V}ar(Y_i))
$$

This means that for large $n$, $\bar{Y}$ is approximately distributed as a $\mathcal{N}(\mathbb{E}(Y_i), \mathbb{V}ar(Y_i)/n)$.
We call $\sqrt{\mathbb{V}ar(Y_i)/n}$ the standard error of $\bar{X}$. For the log-normal we have that 
$\mathbb{V}ar(Y_i) = (e^{\sigma^2} - 1)e^{2\mu + \sigma^2}$.
To see how this approximation works we can extract many random samples, say $R$, of same sample size $n$
and compute the sample mean for each one of them. That is, in the end we end up with a vector with $R$ sample means.
This can be seen as $R$ random draws from the distribution of $\bar{Y}$ and hence they can be used to see how good
the approximation is. If we do this for different sample sizes we can see how the CLT kicks in.


```{r}
ngrid <- c("n100" = 100,"n3000" = 3000)
R <- 10000
Rgrid <- 1:R
mu <- 0
s <- 1

MC <- NULL
for (i in 1:length(ngrid)){
  MC <- cbind(vapply(Rgrid, function(x){
  X <- rnorm(x,0,1)
  Y <- exp(X)
  mY <- mean(Y)},1),MC)
}
colnames(MC) <- names(ngrid)
MC <- as_tibble(MC)
se_true <- sqrt(((exp(s^2) - 1)*(exp(2*mu) + s^2))/ngrid)

c1 <- rgb(0,255,255,max = 255, alpha = 150, names = "lt.blue")
xx <- c(1.5,1.8)
yy <- c(0,15)
br <- 300
lsz <- 0.5
lpos <- "topright"

par(mfrow = c(1,2)) 

hist(MC$n100, breaks = br, xlim = xx, ylim = yy,
     xlab = "", ylab = "", freq = FALSE, col = c1, main = "")
curve(dnorm(x,mean = EY, sd = se_true[1]),
      col = "red", lwd = 2, add = TRUE)
      title(main = "n = 100")
      legend(lpos, legend = c("Simulation","Normal Distr."),
      cex = lsz, col = c(c1,"red"), pch = c(15,NA), lty = c(NA,1))
      abline(v=EY,col="red",lwd=2)


hist(MC$n3000, breaks = br, xlim = xx, ylim = yy,
     xlab = "", ylab = "", freq = FALSE, col = c1, main = "")
curve(dnorm(x,mean = EY, sd = se_true[2]),
      col = "red", lwd = 2, add = TRUE)
abline(v=EY,col="red",lwd=2)
title(main = "n = 3000")
legend(lpos, legend = c("Simulation","Normal Distr."),
       cex = lsz, col = c(c1,"red"), pch = c(15,NA), lty = c(NA,1))

```

We see that as theory predicts the sample mean is unbiased and the normal approximation improves as $n$ grows.

# Ordinary Least Squares

We can go back to our ECV data to run some regressions. To run a regression of log income on sex we write

```{r}
m1 <- lm(log(Y) ~ sex, ecv) 
summary(m1)
```

So we see that the expected wage for males is 3% higher than that of female. However, our standard errors
are not robust to heteroskedasticity. Unfortunately, in R it is a bit more of a hassle to have
robust standard errors than in Stata, still it is not very hard either if we use the library $\texttt{sandwich}$ and
$\texttt{AER}$

```{r}
vcv <- vcovHC(m1, type = "HC1")
coeftest(m1, vcv)
```
So we see the coefficient is significant 5% level. A good exercise is to not use any package. Let $X_i = (1,sex_i)'$, we
know that the ols estimator is

$$
\hat{\beta} = \mathbb{E}_n(X_i X_i')^{-1}\mathbb{E}_n(X_i Y_i) = (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbb{Y},
$$

where $\mathbb{X} = (X_1',...,X_n')$ is called the design or model matrix and in this case has two columns, the first one
filled with ones and the second one is $(sex_1,...,sex_n)'$ and $\mathbb{Y}$ is a column with $(Y_1,...,Y_n)$. We can write our own function s_reg and use it instead of the inbuilt regression package. For now just look at the function s_reg up to where bhat is created, the rest is for the standard errors which will be explained right after.

```{r}
n <- length(ecv$Y)
Y <- log(ecv$Y)
sex <- as.numeric(as.factor(ecv$sex)) - 1
XX <- cbind(rep(1,n),sex)
#number of regressors (including constant)
k <- ncol(XX)

s_reg <- function(X,Y, se = NULL){
  n <- length(Y)
  Y <- as.matrix(Y)
  bhat <- solve(t(X)%*%X)%*%(t(X)%*%Y)
  u <- Y - X%*%bhat
  if (is.null(se) == 1){
    Shat <- solve((1/n)*(t(X) %*% X))*as.numeric((1/(n-k))*(t(u)%*%(u)))
  }
  else{
    B <- 0
    B <- (t(as.matrix(X)*as.numeric(u)))%*%(as.matrix(X)*as.numeric(u))
    Shat <- (solve((1/n)*(t(X) %*% X)) %*% ((1/(n-k)*B)) %*% solve((1/n)*(t(X) %*% X)))
  }
  sehat <- sqrt(Shat[row(Shat) == col(Shat)]/n)
  res <- list("b" = bhat, "se" = sehat)
}

m1 <- s_reg(XX,Y)
print(m1$b)

```

As you see we get exactly the same as with the $\texttt{lm()}$ function. From OLS theory we know that under homoskedasticity

$$
\sqrt{n}(\hat{\beta} - \beta_0) \to_d \mathcal{N}(0, \Sigma), \text{ } \Sigma = \mathbb{E}(X_i X_i')^{-1} \mathbb{E}(u_i^2)
$$
where $u_i = Y_i - X_i' \beta_0$ and $\beta_0 = \mathbb{E}(X_i X_i')^{-1}\mathbb{E}(X_i Y_i)$, an estimator of $\Sigma$ is

$$
\hat{\Sigma} =  \mathbb{E}_n(X_i X_i')^{-1} \mathbb{E}_n(\hat{u}_i^2) = (\mathbb{X}'\mathbb{X})^{-1} (\hat{\mathbb{u}}'\hat{\mathbb{u}})
$$

where $\hat{\mathbb{u}} = (\hat{u}_1,...,\hat{u}_n)'$ and the standard errors are $se = (\sqrt{\Sigma_{11}/n},\sqrt{\Sigma_{22}/n})$
with estimator $\hat{se} = (\sqrt{\hat{\Sigma}_{11}/n},\sqrt{\hat{\Sigma}_{22}/n})$

```{r}
print(m1$se)
```

More generally (without assuming homoskedasticity) we have 

$$
\sqrt{n}(\hat{\beta} - \beta_0) \to_d \mathcal{N}(0, \Sigma), \text{ } \Sigma = \mathbb{E}(X_i X_i')^{-1} \mathbb{E}(X_i X_i' u_i^2) \mathbb{E}(X_i X_i')^{-1}
$$


and

$$
\hat{\Sigma} = \mathbb{E}_n(X_i X_i')^{-1} \mathbb{E}_n(X_i X_i' \hat{u}_i^2) \mathbb{E}_n(X_i X_i')^{-1} = 
\biggl(\frac{1}{n}\mathbb{X}'\mathbb{X}\biggr)^{-1}\biggl(\frac{1}{n}\sum_{i=1}^n u_i^2 X_i X_i'\biggr)\biggl(\frac{1}{n}\mathbb{X}'\mathbb{X}\biggr)^{-1}
$$

For the middle term the matrix form would be $\mathbb{X}'B\mathbb{X}$ where $B = diag(u_1^2,...,u_n^2)$, since this is an nxn matrix 
it will be heavy on the memory of the computer so it is better to just compute the sum with a loop or (even faster) using element by element multiplication: $(1/n)*(\mathbb{X}**u)'*(\mathbb{X}**u)$, where $**$ stands for element by element multiplication (common multiplication in R). Note that in the function s_reg whenever we compute a mean involving residuals we divide by $n-k$ to correct for the degrees of freedom taken by the estimation of the coefficients, however this is asymptotically negligible.


```{r}
m2 <- s_reg(XX,Y,se = "r")
print(m2$b)
print(m2$se)

```

## Boostrapping standard errors

A common way to get standard errors when no explicit expression is available is to use bootstrap. To illustrate let us compute the standard errors of the above regression with bootstrap (even though we do have an explicit expression for them). The key idea is 
to consider the empirical distribution of our data $F_n$ as the "true" distribution and then we can 
take samples from it and get an estimate for each sample. In this way we can approximate the distribution
of our estimator.

```{r}
B <- 1000

b_boot <- sapply(1:B, function(x){
  ecv_boot <- sample(1:n, n, replace = TRUE)
  ecv_boot <- ecv[ecv_boot,]
  Y <- log(ecv_boot$Y)
  sex <- as.numeric(as.factor(ecv_boot$sex)) - 1
  XX <- cbind(rep(1,n),sex)

  m <- s_reg(XX,Y,se = "r")
  return(m$b)})

print(c(sd(b_boot[1,]),sd(b_boot[2,])))

```


# Bias and Coverage rate in Monte Carlo experiments

When an estimator is proposed together with its inferential theory it is common to do a Monte Carlo (MC)
experiment showing the bias and the coverage rate of the estimators. Let us do a MC to see the bias
of OLS and the coverage rate of different standard error estimators. In the MC we take many samples
from the true DGP and for each one we estimate the coefficients and the standard errors. The bias
will be the difference between the mean of all the estimates and the true value. The coverage rate
is the proportion of times that the true value falls inside the confidence interval or, equivalently,
the proportion of times that we do not reject that the coefficient is equal to the true value.

Let us use the following DGP with heteroskedasticity

$$
\begin{align*}
Y_i &= 1 + 3*X_i + u_i \\
u_i &=  \varepsilon_i*X_i \\
X_i, \varepsilon_i &\sim \mathcal{N}(0,1) , \quad X_i \perp \varepsilon_i
\end{align*}
$$
So now $\mathbb{E}[u_i^2|X_i] = \mathbb{E}[\varepsilon^2]*X_i^2 = X_i^2$, so we have heteroskedasticity and note that $\mathbb{E}[X_i*u_i] = \mathbb{E}[X_i*X_i]*\mathbb{E}[\varepsilon_i] = 0$ so we do not have endogeneity.

In the simulation we report the bias, MSE and coverage for the (incorrect) homoskedastic standard errors, the heteroskedasticity robust standard errors and the bootstrap standard errors. 

```{r}
ngrid <- c(100,500,1000)
R <- 1000
B <- 100
Rgrid <- 1:R
btr <- 0.2
set.seed(123)

MC <- NULL
bias <- rep(0,length(ngrid))
mse <- rep(0,length(ngrid))
cov1 <- rep(0,length(ngrid))
cov2 <- rep(0,length(ngrid))
cov3 <- rep(0,length(ngrid))
for (i in 1:length(ngrid)){
  n <- ngrid[i]
  MC <- lapply(Rgrid, function(x){
    X <- rnorm(n)
    XX <- cbind(rep(1,n),X)
    u <- rnorm(n)*X
    Y <- 1 + btr*X + u
    # dff <- as_tibble(cbind(Y = Y, X = X))
    #Point estimate
    m1 <- s_reg(XX,Y)
    b1 <- m1$b[2]
    # b1 <- lm(Y ~ X, dff)
    # b1 <- b1$coefficients[2]
    #Standard errors
    #non heteroskedasticity robust
    se1 <- m1$se[2]
    rej1 <- (abs(b1 - btr)/se1) >= qnorm(0.975) 
    #heteroskedasticity robust
    m2 <- s_reg(XX,Y, se = "r")
    se2 <- m2$se[2]
    rej2 <- (abs(b1 - btr)/se2) >= qnorm(0.975) 
    #Bootstrap
    b_boot <- sapply(1:B, function(x){
      boot <- sample(1:n, n, replace = TRUE)
      boot <- as_tibble(cbind(Y = Y, X = X)[boot,])
      Yb <- boot$Y
      Xb <- boot$X
      XXb <- cbind(rep(1,n),Xb)
      m <- s_reg(XXb,Yb)
      return(m$b[2])})
    se3 <- sd(b_boot)
    rej3 <- (abs(b1 - btr)/se3) >= qnorm(0.975) 
    return(c(b1,rej1,rej2,rej3))})
  MC <- matrix(unlist(MC),4,R)
  bias[i] <- mean(MC[1,]) - btr
  mse[i] <- (mean(MC[1,]) - btr)^2 + var(MC[1,])
  cov1[i] <- 1 - mean(MC[2,])
  cov2[i] <- 1 - mean(MC[3,])
  cov3[i] <- 1 - mean(MC[4,])
}
res <- cbind(Bias = bias, MSE = mse, Homoskedastic = cov1,
             Heteroskedastic = cov2, Bootstrap = cov3)
row.names(res) <- paste("n = ",ngrid)
res


```

As predicted by OLS theory, OLS is unbiased, its accuracy measured by the MSE decreases as n increases. Also, the coverage of the heteroskedasticity robust standard errors is close to the nominal 0.95 while the standard errors computed under the assumption of homoskedasticity are far from 0.95.

# Average Treatment Effects

Suppose that we have some treatment $D_i \in \{0,1\}$ and an observed outcome $Y_i = D_i Y(1)_i + (1 - D_i)*Y(0)_i$ where $Y(D_i)_i$ is a potential outcome: the outcome individual $i$ gets under $D_i$. Since in our data individuals are either treated or not, there is always an element of the pair $(Y(1)_i,Y(0)_i)$ which we do not observe. If the potential outcomes are independent from treatment $A1: (Y(1)_i,Y(0)_i) \perp D_i$ (e.g. if the treatment is randomized) then we can identify what we call the Average treatment effect (ATE):

$$
\tau_0 = \mathbb{E}[Y(1)_i - Y(0)_i] = \mathbb{E}[Y_i | D_i = 1] - \mathbb{E}[Y_i | D_i = 0] ,
$$
where the second equality follows from A1. We can estimate $\tau_0$ by OLS by simply running a regression of $Y_i$ on a constant and $D_i$ and we can take the robust OLS standard errors. It is also common to include other regressors $X_i$ to the regression. Since $D_i$ is randomly assigned, $X_i$ will be independent of $D_i$ and hence will not affect the parameter associated to $D_i$, also, if $X_i$ has explanatory power over $Y_i$ it will increase the precision of our ATE estimator by reducing the variance of the residuals. 

Let us load data used in Lalonde (1986) and Dehejia & Wahba (1999) of a random experiment (National Supported Work (NSW)) where job training is assigned at random among a group of disadvantaged individuals. We also clean the workspace and load some packages we are going to need in this section (we download the package containing the data from github using devtools, check the link for more info).


```{r, results='hide'}
rm(list = ls())
library(devtools)
library(dplyr)
#download package from github see https://github.com/jjchern/lalonde
devtools::install_github("jjchern/lalonde")

exp_df <- as_tibble(lalonde::nsw)
print(exp_df)
```

We are interested in real earnings in 1978 (re78) since in 1978 the program had already finished. Since this data is experimental we can directly estimate the ATE without adjusting for covariates and adjusting for covariates (we adjust for age, age squared, education, race and high school degree)

```{r}
m1exp <- lm(re78 ~ treat, exp_df)
summary(m1exp)
tau1exp <- m1exp$coefficients[2]
m2exp <- lm(re78 ~ treat + poly(age,2) + education + black + hispanic + nodegree, exp_df)
summary(m2exp)
tau2exp <- m2exp$coefficients[2]
print(c("ATE" = tau1exp, "Adjusted ATE" = tau2exp))
``` 
We can see that the unadjusted ATE is 886 dollars, meaning that real earnings of treated individuals in 1978 were 886 dolars higher than the real earnings of the control. When adjusting for covariates the ATE decreases slightly to almost 800 dollars. In both cases we have that the effect is significant at the 10% level.

The main idea in Lalonde (1986) and Dehejia & Wahba (1999) is to replace the experimental control group with a control group which is taken from a survey. This means that we go from an experimental setting to an observational one (one where the potential outcomes cannot be expected to be independent from the treatment). The goal is to see whether observational methods can get near to the experimental estimates. To construct this database we keep only the treated from the experimental data and take as control the Current Population Survey (CPS).

```{r}
cps_lalonde <- lalonde::cps_controls
obs_df <- exp_df %>%
  filter(treat == 1) %>% 
  bind_rows(cps_lalonde) %>% 
  select(-data_id)
```

To be able to identify treatment effects we still need to assume selection on observables: $A1': (Y(1)_i, Y(0)_i) \perp D_i | X_i$. That is, the distribution of the potential outcomes now can vary for individuals with different treatments as long as they have different covariates, however, for those with the same covariates, the potential outcomes still have to be independent from treatment. When the dimension of $X_i$ is large it is handy to define the propensity score:

$$
p(X_i) = \mathbb{P}(D_i = 1 | X_i),
$$
i.e. the probability of being treated given the covariates. This is useful thanks to two results in Rosenbaum & Rubin(1983). Mainly that if selection on observables holds and $A2: 0< p(X_i) < 1$ a.s. (Common Suppport), then $(Y(1)_i, Y(0)_i) \perp D_i | p(X_i)$, i.e. it is enough to control for $p(X_i)$. Also, if $p(X_i)$ is the propensity score then $X_i \perp D_i | p(X_i)$, so the distribution of $X_i$ does not vary across treatment status given the propensity score. One can show that under A1' and A2

$$
\tau_0 = \mathbb{E}\biggl(\frac{D_i Y_i}{p(X_i)} - \frac{(1-D_i)Y_i}{1 - p(X_i)} \biggr).
$$
However, $p(X_i)$ is generally unknown and has to be estimated. To this end we can impose a parametric model. A popular choice is 

$$
p(X_i) = G(X_i' \beta),
$$
where $G(\cdot)$ is the c.d.f of a distribution which is symmetric around 0 and $\beta$ is a vector of parameters. Then the conditional likelihood of $D_i|X_i$ is

$$
f(D_i|X_i; \beta) = G(X_i' \beta)^{D_i}[1-G(X_i' \beta)]^{1-D_i}
$$

and the log-likelihood

$$
l_i(\beta) = D_i  \log G(X_i' \beta) + (1 - D_i) \log[1 - G(X_i' \beta)] 
$$
and $\hat{\beta} = \arg \max_\beta \sum_{i=1}^n l_i(\beta)$. A popular choice of $G(\cdot)$ is the logit model

$$
G(z) = \frac{e^z}{1 + e^z},
$$

for which it is simple to compute the score. To estimate the propensity score $\hat{p}(X_i) = G(X_i' \hat{\beta})$ using a logit model in R we can do the following (we introduce as new covariates age squared, married and earnings in 1975 and its squared following Dehejia and Wahba (1999))

```{r}
logm = glm(treat ~ poly(age,2) + poly(education,2) + married + poly(re75,2) + black + hispanic + nodegree,
             data = obs_df, family = "binomial")
obs_df$pscore <- logm$fitted.values
```


Now, we are ready to estimate $\tau_0$ as

$$
\hat{\tau}^{IPW}_1 = \frac{1}{n}\sum_{i=1}^n \frac{D_i Y_i}{\hat{p}(X_i)} - \frac{1}{n}\sum_{i=1}^n\frac{(1-D_i)Y_i}{1-\hat{p}(X_i)}
$$
Noting that $\mathbb{E}[D_i/p(X_i)] = 1$ by LIE and the same happens with $\mathbb{E}[(1-D_i)/(1-p(X_i))]$, an asymptotically equivalent estimator is

$$
\hat{\tau}^{IPW}_2 = \biggl(\sum_{i=1}^n \frac{D_i}{\hat{p}(X_i)}\biggr)^{-1} \biggl(\sum_{i=1}^n \frac{D_i Y_i}{\hat{p}(X_i)}\biggr) - \biggl(\sum_{i=1}^n \frac{1-D_i}{1-\hat{p}(X_i)}\biggr)^{-1} \biggl(\sum_{i=1}^n\frac{(1-D_i)Y_i}{1-\hat{p}(X_i)}\biggr)
$$
$\hat{\tau}^{IPW}_1$ and $\hat{\tau}^{IPW}_2$ are called Inverse Probability Weight (IPW) estimators and are a difference of weighted sums of $Y_i$. In $\hat{\tau}^{IPW}_2$ the weights sum up to $1$ and we have better finite sample properties. In fact, both estimators are members of the broader class of consistent, semiparametric estimators found in Robins, Rotnitzky and Zhao (1994). In this paper they find the estimator achieving the least asymptotic variance in this class to be none of the above but

$$
\hat{\tau}^{DR} = \frac{1}{n}\sum_{i=1}^n \frac{D_iY_i - m_1(X_i)(D_i - \hat{p}(X_i))}{\hat{p}(X_i)} - \frac{1}{n}\sum_{i=1}^n \frac{(1-D_i)Y_i - m_0(X_i)(D_i - \hat{p}(X_i))}{1 - \hat{p}(X_i)}, 
$$
where $m_j(X_i) = \mathbb{E}[Y_i | D_i = j, X_i]$, $j = 0,1$. This estimator is called Doubly robust because it has the attractive property that if we impose some parametric specification say $m_j(X_i) = m_j(X_i, \alpha_j)$, the estimator is still consistent if $m_j$ is incorrectly specified but the specification of the propensity score is correct and it is also consistent if the specification of the propensity score is not correct but that of $m_j$ is correct. This estimator is also called augmented IPW since it is "augmented" with the outcome regression. Now we are ready to do these estimations in R

```{r}
Y <- obs_df$re78
D <- obs_df$treat
PS <- obs_df$pscore

ipw1 <- mean(D*Y/PS - (1-D)*Y/(1-PS))
ipw2 <- (1/(sum(D/PS)))*sum(D*Y/PS) - (1/(sum((1-D)/(1-PS))))*sum((1-D)*Y/(1-PS))

# Doubly Robust
m1 <- lm(re78 ~ poly(age,2) + education + black + hispanic + nodegree, filter(obs_df,treat == 1))
m1hat <- predict(m1, obs_df)
m0 <- lm(re78 ~ poly(age,2) + education + black + hispanic + nodegree, filter(obs_df,treat == 0))
m0hat <- predict(m0, obs_df)

ipwdr <- mean((D*Y - (D-PS)*m1hat)/PS) - mean(((1-D)*Y - (D-PS)*m0hat)/(1-PS))
c("IPW1" = ipw1, "IPW2" = ipw2, "DR" = ipwdr)
```

We can see that the estimates are not very stable. This can happen when the common support assumption is violated since having estimated propensity scores in the denominators which are close to zero will produce instability. In our case it is very hard to maintain the common support assumption. To see these let us do an histogram with the propensity scores by treatment status.

```{r}
c1 <- rgb(173,216,230,max = 255, alpha = 180, names = "lt.blue")
c2 <- rgb(255,192,203, max = 255, alpha = 180, names = "lt.pink")

par(mar = c(5, 5, 5, 5) + 0.3)
hist(filter(obs_df, treat == 1)$pscore, freq = TRUE, col = c1, axes = FALSE, xlab = "", ylab = "", main = "")
axis(side = 1, xlim = c(0,1))
axis(side = 4, ylab = "")
mtext(side = 4, text = "NSW", line = 2.5, col = "blue")
par(new=TRUE)
hist(filter(obs_df, treat == 0)$pscore, ylim = c(0,16000), freq = TRUE, axes = FALSE, col = c2, xlab = "", ylab = "", main = "Common Support")
axis(side = 2)
mtext(side = 2, text = "CPS", line = 2.5, col = "pink")
```


We see that the control group (the CPS observations) are highly concentrated close to zero. This is because the NSW experiment chose disadvantaged individuals which are not representative of the US population as the CPS is. Hence, we have a comparability issue. A practical solution is to trim the observations with very low propensity scores. For instance, if we trim observation with a propensity score lower than 0.25 we see the Common Support assumption seems more plausible

````{r}
obs_df <- filter(obs_df, pscore > 0.25)

par(mar = c(5, 5, 5, 5) + 0.3)
hist(filter(obs_df, treat == 1)$pscore, freq = TRUE, col = c1, axes = FALSE, xlab = "", ylab = "", main = "")
axis(side = 1, xlim = c(0,1))
axis(side = 4, ylab = "")
mtext(side = 4, text = "NSW", line = 2.5, col = "blue")
par(new=TRUE)
hist(filter(obs_df, treat == 0)$pscore, freq = TRUE, axes = FALSE, col = c2, xlab = "", ylab = "", main = "Common Support")
axis(side = 2)
mtext(side = 2, text = "CPS", line = 2.5, col = "pink")
```


and the estimates become more stable and much closer to the experimental treatment effect.

```{r, warning=FALSE}
Y <- obs_df$re78
D <- obs_df$treat
PS <- obs_df$pscore

ipw1 <- mean(D*Y/PS - (1-D)*Y/(1-PS))
ipw2 <- (1/(sum(D/PS)))*sum(D*Y/PS) - (1/(sum((1-D)/(1-PS))))*sum((1-D)*Y/(1-PS))

# Doubly Robust
m1 <- lm(re78 ~ poly(age,2) + education + black + hispanic + nodegree, filter(obs_df,treat == 1))
m1hat <- predict(m1, obs_df)
m0 <- lm(re78 ~ poly(age,2) + education + black + hispanic + nodegree, filter(obs_df,treat == 0))
m0hat <- predict(m0, obs_df)

ipwdr <- mean((D*Y - (D-PS)*m1hat)/PS) - mean(((1-D)*Y - (D-PS)*m0hat)/(1-PS))
c("IPW1" = ipw1, "IPW2" = ipw2, "DR" = ipwdr)
```


